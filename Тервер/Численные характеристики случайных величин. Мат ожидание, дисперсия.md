#тервер 
## Мат ожидание
Пусть $\xi$ - дискретная [[Случайная величина. Измеримая функция. Основные распределения|случайная величина]], принимающая значения $x_i, \ i \geq 1$, с вероятностями $P(\xi = x_i) = p_i, \ \sum\limits_{i \geq 1}p_i = 1$. Если расписать [[Математическое ожидание (интеграл Лебега)|интеграл Лебега]] по определению, то получим:
$$E_{\xi} = \sum\limits_{i = 1}^{n} x_i P(\xi = x_i)$$
Пример с грузиками:

```handdrawn-ink
{
	"versionAtEmbed": "0.3.3",
	"filepath": "Вставляемые материалы/Ink/Drawing/2025.2.25 - 16.22pm.drawing",
	"width": 500,
	"aspectRatio": 1
}
```
$N = n_1 + \dots + n_k$
Хочется нормировать их вес и использовать нормированные величины: $\frac{n_1}{N}, \ \frac{n_2}{N}$

Мат ожидание играет роль центра масс. Но этот "центр масс" не всегда существует.

Посчитаем мат ожидание для разных величин:
- $Bern(P)$
	$E_{\xi} = 0 \cdot (1 - P) + 1 P = P$
- $\sum\limits_{k = 1}^{\infty} \frac{1}{k^2} = C$
	Построим [[Случайная величина. Измеримая функция. Основные распределения|случайную величину]] $\xi$ такую, что $P(\xi = k) = C^{-1} \cdot \frac{1}{2}$
	$\sum\limits_{k = 1}^{\infty} k \cdot \frac{C^{-1}}{k^2} = C^{-1} \sum\limits_{k = 1}^{\infty} \frac{1}{k}$

## Свойства мат ожидания
Вспомнили [[Математическое ожидание (интеграл Лебега)|индикаторную величину]]: $I_{A}(\omega) = \begin{cases} 1, \ \omega \in A \\ 0, \ \omega \notin A \end{cases}$

1. $E \cdot I_A = P(\xi = A) = P(A)$
2. $E(\xi + \eta) = E \xi + E \eta$ (Для вычисления мат ожидания не нужно смотреть на зависимость/независимость событий)
	Тогда получаем, что формулу можно переписать: $E_{\xi} = \sum\limits_{k = 1}^{n} x_k P(\xi = x_k) = \sum\limits_{w_i} \xi(w_i) \cdot P(w_i)$
3. $E_{c \cdot \xi} = c \cdot E_{\xi}$
4. $Eg(\xi) = \sum\limits_{k = 1}^{} g(x_k) \cdot P(\xi = x_k)$
	Пример: хотим посчитать мат ожидание от $x^2: \ E \xi^2 = \sum\limits_{k = 1} x_k^2 \cdot P(\xi = x_k)$
5. Матожидание от константы равно константе: $Ec = c$

## Дисперсия
$$D \xi = E(\xi - E \xi)^2$$
Есть эквивалентное определение: 
$D \xi = E(\xi^2 - 2 \xi E \xi + (E \xi)^2) = E \xi^2 - 2 E(\xi E \xi) + E((E \xi)^2) =$
$= E \xi^2 - 2 E\xi \cdot E \xi + (E \xi)^2 = E \xi^2 - (E \xi)^2$, то есть:
$$E \xi^2 - (E \xi)^2$$
Что это вообще такое? Вернёмся к примеру с гирьками:
Положи мы одинаковые грузики на точки 10, -10 или 100, -100, центр масс будет в одном и том же месте.

Пример с гирей и грифом: чем вес ближе к центру масс, тем меньше момент инерции (легче вращать предмет).
$$\sum(x_k - a)^2 P(x_k)$$

Преобразуем формулу **Дисперсии**:
$E(\xi - E \xi)^2 = \sum\limits_{x_k} (x_k - E \xi)^2 P(\xi = x_k)$

Вывод: **Чем меньше дисперсия, тем ближе все значения к мат ожиданию**

## Свойства дисперсии
1. $$D \xi \geq 0$$
	$D\xi = 0 \iff P(\xi = x_k) = 1$
2. $$D c \cdot \xi = c^2 \cdot D \xi$$
3. Если $\xi$ и $\eta$ **независимы**, то $D(\xi + \eta) = D\xi + D\eta$
	Пример: 
	- пусть $\xi$ - количество "орлов" при подбрасывании n монеток
	- пусть $\eta$ - количество "решек" при подбрасывании n монеток
	$D(\xi + \eta) = D(n) = 0$ так как n - константа
4. $$D(\xi + \eta) = E(\xi + \eta - E(\xi + \eta))^2 = D \xi + D \eta + 2 E((\xi - E \xi) \cdot (\eta - E \eta))$$
	**Ковариация случайных величин $\xi \ и \ \eta: \ cov(\xi, \eta)$**
	$$cov(\xi, \eta) = E((\xi - E \xi) \cdot (\eta - E \eta))$$
	Для зависимых величин ковариация = 0
	Ковариация показывает ненормированную зависимость между двумя величинами.

## Задачи
### №1
**Условие:**
$P(X = 0) = P(X = 1) = \frac{1}{2}$
$P(Y = 1000) = 1 - P(Y = 0) = 0.0005$
Найти $EX, DX, EY, DY$

**Решение:**
1. $EX = \sum x_k P(x = x_k) = 0 + 1 \cdot \frac{1}{2} = \frac{1}{2}$
2. $DX = E(X - EX)^2 = E(X - \frac{1}{2})^2 = E(X^2) - (EX)^2 = \frac{1}{4}$
3. $EY = \sum Y_k \cdot P(Y = Y_k) = 0 + 1000 \cdot 0.0005 = \frac{1}{2}$
4. $DY = E(Y - EY)^2 = E(Y^2) - (EY)^2 = 500 - \frac{1}{4}$

### №2
**Условие:**
Найти матожидание биномиальной случайной величины

**Решение:**
$C_n^k p^k(1 - p)^{n - k}$

$\eta = E(\sum\limits_{i = 1}^{n} \xi_i) = \sum\limits_{i = 1}^{n} (E \xi_i) = \sum\limits_{i = 1}^{n} p = n \cdot p$

### Задача не из списка
а) $D \xi$, где $\xi = Bern(p)$
	$D \xi = E \xi^2 - (E \xi)^2 = p - p^2 = p(1 - p)$
	$g(x) = x^2$
	$E \xi^2 = \sum\limits_{x_k} x_k^2 P(x_k = \xi) = 1 \cdot p = p$
б) $D \eta$, где $\eta = Binom(n, p)$
	$D \eta = D \xi \cdot n = np(1 - p)$

### №3
**Условие:**
Найти матожидание геометрической величины

Решение:
![[Pasted image 20250225174032.png|600]]

### ДЗ:
4, 5, 7, 8, 9