#матстат 
## Множества оценок
![[Pasted image 20251006113854.png|500]]

Что-то поделаем с нерегулярной моделью:
$x_i \sim R[0, \theta]$
$f_{\theta}(x_1) = \frac{1}{\theta} \cdot I(x \in [0, \theta])$
$I_{\theta}(x_1) = E(\frac{\partial}{\partial \theta} \ln f_{\theta}(x_1))^2 = E(\frac{\partial}{\partial \theta} \ln \frac{1}{\theta} \cdot I(x \in [0, \theta]))^2$
- $x \in [0, \theta]: \ E(\theta \cdot (-1) \cdot \theta^{-2})^2 = E (- \theta^{-1})^2 = E \theta^{-2} = \frac{1}{\theta}$
- $x \notin [0, \theta]: \ I_{\theta} = 0$

Из [[Эффективные оценки|неравенства Рао-Крамера]] следует:
$D \hat{\theta} \geq \frac{1}{n I_{\theta}(x)} = \frac{\theta}{n}$

## Достаточные статистики
$\hat{\theta}(x_1, \dots, x_n)$ - **достаточная** для параметра $\theta$, если $P(x_1, \dots, x_n \in B | \ \hat{\theta}(\vec{x}) = K)$ не зависит от $\theta$

Возьмём Бернуллевскую модель:
$Bern(\theta), \ x_1, \dots, x_n$
$\theta = E x_1$
$D x_1 = \theta(1 - \theta)$

Хотим представить $L(\vec{x}, \theta)$ в виде $h(x_1, \dots, x_n) \cdot \psi(\hat{\theta}(\vec{x}), \theta)$

$L(x_1, \theta) = P_{\theta}(\xi_1 = x_1) = \begin{cases} \theta, \ если x_1 = 1 \\ 1 - \theta, \ если x_1 = 0 \end{cases} = \theta^{x_1} \cdot (1 - \theta)^{1 - x_1}$

$L(\vec{x}, \theta) = \prod\limits_{i = 1}^{n} L(x_i, \theta) = \prod\limits_{i = 1}^{n} \theta^{x_i} \cdot (1 - \theta)^{1 - x_i} = \theta^{\sum x_i} (1 - \theta)^{n - \sum x_i} = \theta^{n \vec{x}} \cdot (1 - \theta)^{n - n\vec{x}}$

$h(x_1, \dots, x_n) = 1$
$\hat{\vec{x}} = n \vec{x}$
$\psi(y, \theta) = \theta^{y} \cdot (1 - \theta)^{n - y}$

Тогда получим, что:
$h(x_1, \dots, x_n) \cdot \psi(\hat{\theta}(x), \theta) = L(\vec{x}, \theta) = \theta^{n \vec{x}} \cdot (1 - \theta)^{n - n \vec{x}}$

## Полная достаточная статистика
Статистика $\hat{\theta}(x)$ - **полная**, если из $E g(\theta(x)) = 0$ следует, что $g(\hat{\theta}(\vec{x})) = 0$ для всех $\theta$

## Задача
$x_i \sim Bern(\theta)$
Найти оптимальную оценку для $\theta \cdot (1 - \theta) = D x_i$

1. Найдём достаточную статистику по [[Достаточные статистики|критерию факторизации]]
	Ответ: $n \vec{x}$
2. Докажем, что эта статистика будет полной
	Заметим, что $n \vec{x} = \sum\limits_{i = 1}^{n} x_i \sim Bin(n, \theta)$
	Тогда $Eg(\hat{\theta}(\vec{x})) = \sum\limits_{k = 0}^{n} g(k) \cdot P(\hat{\theta}(x) = k) = \sum\limits_{k = 0}^{n} g(k) \cdot C_n^k \cdot \theta^k \cdot (1 - \theta)^{n - k} = 0$
	Пусть $z = \frac{\theta}{1 - \theta} \implies E g(\hat{\theta}(\vec{x})) = \sum\limits_{k = 0}^{n} g(k) \cdot C_n^k \cdot z^k \cdot (1 - \theta)^n = 0 \implies g(k) = 0 \ для \ k = 0, 1, 2, \dots$
	$z \in (0, + \infty)$

## Домашка
### Задача 1
Найти оптимальную оценку для $\theta \cdot (1 - \theta)$
Спойлер: ищем её из уравнения несмещённости: $E g(\hat{\theta}(\vec{x})) = \theta \cdot (1 - \theta) \implies g$ - оптимальная оценка $\theta \cdot (1 - \theta)$

Решение:
$T = n\vec{x} = \sum\limits_{i = 1}^{n} x_i$
$\xi: \ E \xi(T) = \theta(1 - \theta)$
$n = 2: \ \xi(x_1, x_2) = E(x_1(1 - x_2)) = \theta(1 - \theta)$
$\forall n: \ \xi(x_1, \dots, x_n) = \frac{1}{n(n - 1)} \cdot \sum\limits_{i \neq j} x_i(1 - x_i) = \sum\limits_{i \neq j} x_i - \sum\limits_{i \neq j} x_i x_j = \sum\limits_{i = 1} \sum\limits_{j = 1} x_i x_j =$
$= \sum\limits_{i = 1} x_i^2 + \sum\limits_{i \neq j} x_i x_j = \sum\limits_{i \neq j}^{n} x_i - ((\sum\limits_{i = 1}^{n} x_i)^2 - \sum\limits_{i = 1}^{n} x_i^2) = (n - 1) \sum\limits_{i = 1}^{n} x_i - (\sum\limits_{i = 1}^{n} x_i)^2 + \sum\limits_{i = 1}^{n} x_i =$
$= n \sum\limits_{i = 1}^{n}x_i - (\sum\limits_{i = 1}^{n} x_i)^2$
$\frac{n \sum x_i - (\sum x_i)}{n(n - 1)} = \frac{n T - T^2}{n(n - 1)}$

Другое решение:
$ET = n\theta$
$DT = n \theta(1 - \theta) = ET^2 - (ET)^2$
$n\theta(1 - \theta) = ET^2 - n^2\theta^2$
$ET^2 = n \theta(1 - \theta) + n^2 \theta^2$
$\theta(1 - \theta) = \theta - \theta^2 = \frac{ET}{n} - \frac{ET(T - 1)}{n(n - 1)}$
$h(T) = \frac{T}{n} - \frac{T(T - 1)}{n(n - 1)} = \frac{T(n - T)}{n(n - 1)}$
$ET(n - T) = nET - ET^2 = n(n - 1) \theta (1 - \theta)$
$E \frac{T(n - T)}{n(n - 1)} = \theta(1 - \theta)$

Ещё одно решение:
$\theta(1 - \theta) = \theta - \theta^2$
Подберём оценки для каждого члена и по свойству матожидания 
- Ищем $g_1(T): \ Eg_1(T) = \theta \implies g_1(T) = \frac{T}{n}$
- Ищем $g_2(T): \ Eg_2(T) = \theta^2$
	$g_2(T) = aT + b: \ Eg_2(T) = \theta^2 \implies EaT + b = a \cdot \theta + b \neq \theta^2$
	$g_2(T) = aT^2 + bT + c \implies EaT^2 + bT + c = \theta^2$
Ответ: $(g_1 - g_2)(T)$

### Задача 2
$x_1, \dots, x_n \sim N(\theta, 1)$
$x_0$ - некоторое число
Нужно найти оптимальную оценку для $\tau(\theta) = P_{\theta}(x_1 \leq x_0)$

Решение:
Сначала найдём достаточную статистику
$L(\vec{x}, \theta) = \frac{1}{(\sqrt{2 \pi})^n} \cdot e^{- \frac{1}{2} \sum\limits_{i = 1}^{n} (x_i - \theta)^2} = \frac{1}{(\sqrt{2 \pi})^n} \cdot e^{- \frac{1}{2} \sum x_i^2} \cdot e^{\theta \sum x_i - \frac{n \theta^2}{2}}$
$L(\vec{x}, \theta) = h(x) \cdot g(S(x), \theta)$
Тогда $\sum x_i = Ы(ч)$ - достаточная статистика

$E \varphi(S(x)) = 0 \iff \varphi(S(x)) = 0 \ \forall \theta$
$\int\limits_{- \infty}^{\infty} \varphi(y) \cdot f_{S(x)}(y) dy$

$\sum x_i \sim N(n \cdot \theta, n)$
$x_i \sim N(\theta, 1)$

Воспользовались теоремой из [[Семинар 13.10.25]]
$\varphi(S(x)): \ E \varphi(S(x)) = P_{\theta}(x_1 \leq x_0) = E I(x_1 \leq x_0)$

$\hat{\theta}(z) = E(I(x_1 \leq x_0 | \sum x_i))$

## Задача 1
$x_1, x_2, \dots, x_n$
$f_{x_1}(x, \theta) = e^{-x + \theta} \cdot I(x > \theta)$
Надо найти оптимальную оценку для $\theta$

Решение:
1. Напишем функцию правдоподобия
	$L(\vec{x}, \theta) = e^{- \sum x_i + n \cdot \theta} \cdot \prod\limits_{i = 1}^{n} I(x_i > \theta) = e^{- \sum x_i} e^{n \cdot \theta} I(x_{(1)} > \theta)$
2. Понимаем, что достаточной статистикой тут будет $x_{(1)}$, так как функция правдоподобия выражается через произведение ([[Достаточные статистики|критерий факторизации]])
3. 

## Домашнее задание
### Задача 1
$E(\xi | \eta) = E \xi$ - почему?????????
$\xi, \ \eta$ - независимые

### Задача 2
$\xi \sim N(a_1, \sigma_1^2), \eta \sim N(a_2, \sigma_2^2)$
Когда $\xi, \eta$ - независимые? (когда ковариация равна 0)

### Задача 3
Когда $x_1 - c \sum x_i$ независима с $\sum x_i$?
Надо приравнять ковариацию 0 и найти c

$E(I(x_1 \leq x_0) | \sum x_i)$
Пусть $\sum x_i = y$
$E(I(x_1 - c \cdot \sum x_i \leq x_0 - c \cdot y)\  |\ \sum x_i = y) = E(I(x_1 - c \cdot \sum x_i \leq x_i - c \cdot y)) = P(x_1 - c \cdot \sum x_i \leq x_0 - c \cdot y)$

## Задача 4
Проверить полноту статистики $x_1$ из **задачи 1**
Либо через критерий ($E \varphi(S(x)) = 0 \iff \varphi(S(x)) = 0, \ \int\limits_{- \infty}^{\infty} \varphi(y) \cdot f_{x_{(1)}}(y) dy = 0$)

## Задача 5
Найти $f_{x_{(1)}}(y)$
$\int\limits_{0}^{\infty} \varphi(y) \cdot f_{x_{(1)}}(y) dy = 0$
Надо взять производную по $\theta$