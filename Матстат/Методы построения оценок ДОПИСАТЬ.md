#матстат 

# Теоремы
## Теорема о полноте достаточной статистики
Пусть $\mathcal{F} = \{ F(x, \theta), \ \theta \in \Theta \}$ - регулярное экспоненциальное и $f(x, \theta) = exp\{ A(\theta) B(x) + C(\theta) + D(x) \}$.

Если у $A(\theta)$ для всех $\theta \in \Theta$ найдётся отрезок значений (набор значений, образованный непрерывным отрезком), тогда [[Достаточные статистики|достаточная статистика]] $T(\vec{x}) = \sum B(x_i)$ будет полной.

## Теорема об оценке, полученной методом максимального правдоподобия
Пусть $\mathcal{F} = \{ F(x, \theta), \ \theta \in \Theta \}$ - регулярное и экспоненциальное.
Если:
1) $\frac{\partial L}{\partial \theta}, \ \frac{\partial^2 L}{\partial \theta^2}, \ \frac{\partial^3 L}{\partial \theta^3}$
	$H_1(\vec{x}), \ H_2(\vec{x}), \ H_3(\vec{x})$
	$|\frac{\partial L}{\partial \theta}| < H_1(\vec{x})$
	$|\frac{\partial^2 L}{\partial \theta^2}| < H_2(\vec{x})$
	$|\frac{\partial^3 L}{\partial \theta^3}| < H_3(\vec{x})$
	$\int H_i L(\vec{x}, \theta) d \vec{x} < M$
2) $i_n(\theta) = \int (\frac{\partial \ln L}{\partial \theta})^2 L(\vec{x}, \theta) d \vec{x} < \infty$

Тогда:
$\theta_{О. М. П.}$ - оценка, полученная методом максимального правдоподобия, обладает:
1) [[Точечные оценки. Несмещённость, состоятельность|Состоятельностью]]
2) $\mathcal{L} (\hat{\theta}_{О. М. П}) \to_{n \to \infty} N(\theta, \frac{1}{n \cdot i_n(\theta)})$ - оценка максимального правдоподобия будет асимптотически-нормальной

# Непосредственно методы
## Метод максимального правдоподобия
$L(\vec{x}, \theta), \ \vec{y} = y_1, \dots, y_n$
$L(\vec{y}, \theta)$ - функция, зависящая от $\theta$

Ищем аргумент $\hat{\theta}$, при котором $L(\vec{y}, \hat{\theta}) = max_{\theta} L(\vec{y}, \theta)$
$\frac{\partial L(\vec{y}, \theta)}{\partial \theta} = 0$

$f(x, \theta)$ - экспоненциальное семейство
$f(x, \theta) = exp\{ A(\theta) B(x) + C(\theta) + D(x) \}$
$L(\vec{x}, \theta) = \prod\limits_{i = 1}^{n} f(x_i, \theta) = e^{A(\theta) \sum B(x_i)} \cdot e^{n C(\theta)} \cdot e^{\sum D(x_i)}$

Берём производную:
$\frac{\partial L}{\partial \theta} = e^{\sum D(x_i)} \cdot e^{n C(\theta)} \cdot (n C'(\theta)) \cdot e^{A(\theta) \sum B(x_i)} + e^{\sum D(x_i)} \cdot e^{n C(\theta)} \cdot (A'(\theta) \sum B(x_i)) \cdot e^{A(\theta) \sum B(x_i)} = 0$

$e^{A(\theta) \sum B(x_i)} \cdot e^{n C(\theta)} \cdot e^{\sum D(x_i)} \cdot (n C'(\theta) + A'(\theta) \sum B(x_i)) = 0$
$ДОПИСАТЬ$
$\frac{1}{n} \sum B(x_i) = \tau(\theta)$

### Пример
$f(x, \theta) = \theta x^{- (\theta + 1)}, \ x \geq 1, \ \theta > 0$
$L(\vec{x}, \theta) = \theta^n e^{-(\theta + 1) \sum\limits_{i = 1}^{n} \ln x_i}$
Возьмём логарифм от этой функции:
$\ln L = n \ln \theta - (\theta + 1) \sum\limits_{i = 1}^{n} \ln x_i$
$\frac{\partial \ln L}{\partial \theta} = \frac{n}{\theta} - \sum\limits_{i = 1}^{n} \ln x_i = 0$
$\hat{\theta}_{О. М. П.} = \frac{n}{\sum\limits_{i = 1}^{n} \ln x_i}$ - статистика, полученная методом максимального правдоподобия

## Метод моментов
$\vec{x}, \ S^2, \ \vec{A_k}, \ \mu(\theta) = E \xi, \ D \xi = \sigma(\theta), \ E \xi^k = a_k(\theta)$
$\vec{x} = \mu(\theta)$
$\begin{cases} \vec{A_{k_1}} = a_{k_1}(\theta) \\ \vdots \\ \vec{A_{k_t}} = a_{k_t}(\theta) \end{cases}$

Пусть $x_1, \dots, x_n$
$\vec{x} = \frac{1}{n} \sum\limits_{i = 1}^{n} x_i$
$S^2 = \frac{1}{n} \sum\limits_{i = 1}^{n} (x_i - \vec{x})^2$
$\vec{A_k} = \frac{1}{n} \sum x_i^k$
$\vec{M_k} = \frac{1}{n} \sum\limits_{i = 1}^{n} (x_i - \vec{x})^k$
$\vec{A_k} = a_k(\theta)$ - теоретический момент
$\vec{M_k} = E(\xi - E \ x_i)^k = S_k(\theta)$

### Утверждение
Решение уравнения (системы) относительно $\theta$ будет **состоятельной оценкой**

### Пример
Пусть есть непрерывное распределение в $R[- \theta, \theta]$
Построим $\hat{\theta}_{М. М.}$
$f(x, \theta) = \frac{1}{2 \theta}, \ x \in [- \theta, \theta]$
$E \xi = 0$
$E \xi^2 = \int\limits_{- \theta}^{\theta} \frac{x^2}{2 \theta} dx = \frac{1}{6 \theta} (\theta^3 + \theta^3) = \frac{\theta^2}{3}$
$\frac{1}{n} \sum\limits_{i = 1}^{n} x_i^2 = \frac{\theta^2}{3}$
$\hat{\theta}_{М. М.} = \sqrt{\frac{3}{n} \sum\limits_{i = 1}^{n} x_i^2}$

# Матожидание выборочной дисперсии
$S^2 = \frac{1}{n} \sum\limits_{i = 1}^{n} (x_i - \vec{x})^2 = \frac{1}{n} \sum\limits_{i = 1}^{n} (x_i^2 - 2 x_i \vec{x} + (\vec{x})^2)$
$E S^2 = \frac{1}{n} \sum E x_i^2 - E(\overline{x})^2$
$E(\frac{1}{n} \sum\limits_{i = 1}^{n} x_i) \cdot (\frac{1}{n} \sum\limits_{i = 1}^{n} x_i) = \frac{1}{n^2}$
![[Pasted image 20251020104223.png]]
![[Pasted image 20251020104258.png]]
