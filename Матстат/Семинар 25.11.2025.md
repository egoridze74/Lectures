#матстат 
## Разбор домашки
### Номер 1
[[Асимптотические доверительные интервалы|ОМП]] (оценка максимального правдоподобия) для $\theta$ в $Exp(\theta)$

$Exp(\theta) = \begin{cases} 1 - e^{- \theta x}, \ x \geq 0 \\ 0, \ иначе \end{cases}$
$fx_i(x) = \theta \cdot e^{- \theta x}$
$L(\vec{x}, \theta) = \prod\limits_{i = 1}^{n} \theta e^{- \theta x_i} = \theta^n \cdot e^{- \theta \sum\limits_{i = 1}^{n} x_i}$
$\ln L(\vec{x}, \theta) = \ln \theta^{n} + \ln e^{- \theta \sum \limits_{i = 1}^{n} x_i} = n \ln \theta - \theta \sum\limits_{i = 1}^{n} x_i$
$\frac{d \ln L}{d \theta} = \frac{n}{\theta} - \sum\limits_{i = 1}^{n} x_i$
$\frac{n}{\theta} - \sum\limits_{i = 1}^{n} x_i = 0 \implies \hat{\theta}_{ОМП} = \frac{n}{\sum\limits_{i = 1}^{n} x_i} = \frac{n}{\frac{\sum\limits_{i = 1}^{n} x_i \cdot n}{n}} = \frac{1}{\vec{x}}$

### Номер 2
$R[0; 1]$
$fx_i = \frac{1}{1 - \theta} \cdot I(x_i \in [0; 1])$
Если решать через ОМП, то $\hat{\theta}_{ОМП} = x_{(n)}$
$L(\vec{x}, \theta) = \prod\limits_{i = 1}^{n} f(x_i, \theta) = \begin{bmatrix} \frac{1}{(1 - \theta)^n} \\ 0, \ \exists \ i: \ x_i \notin [\theta; 1] \end{bmatrix}$
1) $\theta > x_1 \implies L(\vec{x}, \theta) = 0$, так как $x_1 \in [\theta; 1]$
2) $\theta > x_2 \implies L(\vec{x}, \theta) = 0$ и т.д.
![[Pasted image 20251125111417.png|300]]
$\implies \max L(\vec{x}, \theta)$ в $x_{(1)} \implies \hat{\theta}_{ОМП} = x_{(1)}$

### Номер 3
$x_i: \ Fx_i(x) = 1 - e^{- \frac{x^2}{\theta}}, \ x \geq 0$
$fx_i(x) = \frac{2x}{\theta} e^{- \frac{x^2}{\theta}}, \ x \geq 0, \ \theta > 0$
$L(\vec{x}, \theta) = \prod\limits_{i = 1}^{n} \frac{2x_i}{\theta} \cdot e^{- \frac{x^2_i}{\theta}} = (\frac{2}{\theta})^n \cdot \prod\limits_{i = 1}^{n} x_i \cdot e^{- \frac{x^2_i}{\theta}}$
$\ln L(\vec{x}, \theta) = n \ln 2 - n \ln \theta + \ln x_i + \dots + \ln x_n + \ln e^{-\frac{1}{\theta} \sum\limits_{i = 1}^{n} x_i^2}$
$\frac{d \ln L}{d \theta} = - \frac{n}{\theta} + \frac{1}{\theta^2} \sum\limits_{i = 1}^{n} x_i = 0 \implies - \frac{n}{\theta} + \frac{1}{\theta^2} \sum\limits_{i = 1}^{n} x^2 = 0 = - n \theta + \sum\limits_{i = 1}^{n} x_i^2 = 0$
$n \theta = \sum\limits_{i = 1}^{n} x_i^2 \implies \hat{\theta}_{ОМП} = \frac{\sum\limits_{i = 1}^{n} x_i^2}{n}$

### Номер 4
$R[0; \theta]$
$f(x) = \begin{cases} \frac{1}{\theta}, \ x \in R[0; \theta] \\ 0, \ иначе \end{cases}$
$Ex = \frac{\theta}{2}$
$\vec{x} = \frac{1}{n} \sum\limits_{i = 1}^{n} x_i$
$\frac{1}{n} \sum\limits_{i = 1}^{n} x_i = Ex \implies \hat{\theta}_{ОМП} = 2 \vec{x}$
$\frac{\sum\limits_{i = 1}^{n} x_i^k}{n}$ - оценка $E \xi^k$
$E\xi^k = g(\theta)$
$g^{-1}(\frac{\sum x_i^k}{n}) = \hat{\theta}_{ОМП}$

$Ex = \frac{\theta}{2} \implies g(z) = \frac{z}{2} \implies g(\theta) = \frac{\theta}{2} \implies g^{-1}(\theta) = 2 \theta$
$g^{-1}(\frac{\sum x_i}{n}) = 2 \vec{x}$

### Номер 5
$Gamma(\theta_1,\ \theta_2)$
$f(x) = \frac{\theta_2^{\theta_1}}{Г(\theta_1)} \cdot x^{\theta_1 - 1} e^{- \theta_2 x}, \ x > 0$
$Ex = \frac{\theta_1}{\theta_2}$
$Dx = \frac{\theta_1}{\theta_2^2}$

1-ый момент: $\vec{x} = \frac{1}{n} \sum\limits_{i = 1}^{n} x_i$
дисперсия: $\frac{1}{n} \sum\limits_{i = 1}^{n} x_i^2 - (\vec{x})^2$
Тогда:
$\begin{cases} \frac{\theta_1}{\theta_2} = \vec{x} \\ \frac{\theta_1}{\theta_2^2} = \frac{1}{n} \sum\limits_{i = 1}^{n} x_i^2 - (\vec{x})^2 \end{cases}$ или $\begin{cases} \frac{\theta_1}{\theta_2} \\ \frac{\theta_1(1 + \theta_1)}{\theta_2^2} = \frac{1}{n} \sum\limits_{i = 1}^{n} x_i^2 - \ второй \ момент\end{cases}$

### Номер 6
$f(x_i; \theta) = e^{- \frac{x}{\theta}}, \ x > 0$
$E x_i = \int\limits_{0}^{+ \infty} xe^{- \frac{x}{\theta}}dx = \begin{vmatrix} u = x \implies du = dx \\ dv = e^{- \frac{x}{\theta}} \implies v = - \theta e^{- \frac{x}{\theta}} \end{vmatrix} = \theta^2 \implies g(\theta) = \theta^2$
$\vec{x} = g(\theta) \implies \hat{\theta}_{ОММ} = g^{-1}(\vec{x}) = \sqrt{\vec{x}}$
$Ex = \vec{x}$
$\theta^2 = \vec{x}$
$\hat{\theta}_{ОММ} = \sqrt{\vec{x}}$